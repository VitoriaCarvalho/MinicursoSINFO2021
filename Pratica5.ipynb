{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from PIL import Image\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow import keras\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "facenet_model = keras.models.load_model(\"facenet_keras.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sales\\anaconda3\\envs\\Mini\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.22.2.post1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\Sales\\anaconda3\\envs\\Mini\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.22.2.post1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp_model = pickle.load(open('mlp_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo a descrição e a cor dos labels para serem usados na OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_description = {\n",
    "    0: 'NO MASK',\n",
    "    1: 'MASK'\n",
    "}\n",
    "\n",
    "color = {\n",
    "    0: (0, 0, 255),\n",
    "    1: (0, 255, 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faces(image, size=(160, 160)):\n",
    "\n",
    "    '''\n",
    "      Esta função é responsável por extrair as faces de uma imagem através do modelo MTCNN.\n",
    "      Parâmetros:\n",
    "        * image: uma imagem de qualquer dimensão;\n",
    "        * size: refere-se ao shape de redimensionamento das faces, o padrão é (160, 160).\n",
    "      Retorno: uma lista de imagens (faces) do tipo Image Pillow. Caso nenhuma face tenha sido detectada, o retorno é uma lista vazia.\n",
    "    '''\n",
    "\n",
    "    # Transformando imagem em um array numpy\n",
    "    img = np.asarray(image)\n",
    "\n",
    "    # Capturando as faces da imagem através da MTCNN\n",
    "    results = detector.detect_faces(np.asarray(image))\n",
    "\n",
    "    # Lista para armazenar as faces\n",
    "    faces = []\n",
    "\n",
    "    # Percorrendo a lista de faces detectadas\n",
    "    for i in range(len(results)):\n",
    "      try:\n",
    "        # Caso a face tenha sido detectada com mais de 95% de certeza, essa condição é verdadeira\n",
    "        if results[i]['confidence'] > 0.95:\n",
    "\n",
    "          # Extraindo os pontos da face\n",
    "          x1, y1, w, h = results[i]['box']\n",
    "          x2, y2 = x1 + w, y1 + h\n",
    "\n",
    "          # Extraindo a face da imagem fazendo slice nos pontos identificados pela MTCNN\n",
    "          # face = image[y1:y2, x1:x2]\n",
    "\n",
    "          # Adicionando a face encontrada e suas informações na lista de faces\n",
    "          faces.append({\n",
    "              'x1': x1,\n",
    "              'y1': y1,\n",
    "              'x2': x2,\n",
    "              'y2': y2,\n",
    "              # 'face': np.array(Image.fromarray(face).resize(size)),\n",
    "              'confidence': results[i]['confidence']\n",
    "          })\n",
    "\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(face, facenet_model):\n",
    "\n",
    "  '''\n",
    "    Esta função é responsável por receber uma imagem e fornecer os embeddings da mesma, através do modelo FaceNet.\n",
    "    Parâmetros:\n",
    "      * face: é uma imagem com dimensão 160x160, referente a uma face extraída de uma imagem;\n",
    "      * facenet_model: é uma instância do modelo FaceNet previamente treinado, responsável por gerar os embeddings da imagem.\n",
    "    Retorno: um array numpy com 128 posições, referentes às 128 características da FaceNet (embeddings).\n",
    "  '''\n",
    "\n",
    "  # Convertendo a imagem para numpy e normalizando para o intervalo [0..1]\n",
    "  img = np.array(face).astype('float32')/255\n",
    "\n",
    "  # Expandindo a dimensão da imagem para adequá-la à entrada da FaceNet\n",
    "  # O shape deve ficar (1, 160, 160)\n",
    "  input = np.expand_dims(img, axis=0)\n",
    "\n",
    "  # Fazendo a predição do modelo para extrair os embeddings da imagem\n",
    "  embedding = facenet_model.predict(input)[0]\n",
    "\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(embedding, mlp_model):\n",
    "\n",
    "    '''\n",
    "        Esta função é responsável por realizar a predição do modelo de classificação a partir das características de entrada.\n",
    "        Parâmetros:\n",
    "            * embedding: vetor de características de uma face, representado por um array numpy com 128 valores;\n",
    "            * mlp_model: modelo treinado do classificador MLP.\n",
    "        Retorno: um variável label, que terá valor 0 (maskoff) ou 1 (maskon).\n",
    "    '''\n",
    "\n",
    "    # Expandindo dimensão do array para adequá-lo à entrada do classificador\n",
    "    # A dimensão deve ficar (1, 128)\n",
    "    embedding = np.expand_dims(embedding, axis=0)\n",
    "\n",
    "    # Realizando a predição da probabilidade de acerto para cada classe\n",
    "    proba = mlp_model.predict_proba(embedding)\n",
    "\n",
    "    # Extraindo do vetor de probabilidades o label que apresentou o maior resultado\n",
    "    label = np.argmax(proba)\n",
    "\n",
    "    # Caso o modelo tenha menos de 95% de certeza sobre o maior resultado, o label predito receberá o valor inverso\n",
    "    # Por exemplo, se o modelo não tiver 95% ou mais de certeza sobre classificar como label 1, o retorno será label 0\n",
    "    if proba[0][label] < 0.95:\n",
    "        label = abs(label-1)\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_points_in_frame(frame):\n",
    "\n",
    "    '''\n",
    "        Este método é responsável por receber um frame (imagem), detectar as faces existentes, classificá-las de acordo com o uso da máscara e marcar sua categorização visivelmente através da OpenCV.\n",
    "        Parâmetro:\n",
    "            * frame: uma imagem de qualquer dimensão, pode ser um frame de um vídeo.\n",
    "        Retorno: uma imagem com marcações feitas através da OpenCV.\n",
    "    '''\n",
    "\n",
    "    # Transformando a imagem em array numpy\n",
    "    w, h = 160, 96\n",
    "    img = np.asarray(frame)\n",
    "    img_rgb = img[..., ::-1]\n",
    "    img_resize = cv.resize(img_rgb, (w, h))\n",
    "    \n",
    "    # Extraindo faces da imagem\n",
    "    faces = get_faces(img_resize)\n",
    "\n",
    "    # Iniciando contador responsável por marcar quantas pessoas sem máscara existem na imagem\n",
    "    no_mask_count = 0\n",
    "\n",
    "    # Percorrendo a lista de faces identificadas\n",
    "    for face in faces:\n",
    "        # Extraindo os pontos da face\n",
    "        x1 = int((face['x1']/w) * frame.shape[1])\n",
    "        x2 = int((face['x2']/w) * frame.shape[1])\n",
    "        y1 = int((face['y1']/h) * frame.shape[0])\n",
    "        y2 = int((face['y2']/h) * frame.shape[0])\n",
    "\n",
    "        # Extraindo os embeddings da face\n",
    "        face = frame[y1:y2, x1:x2]\n",
    "        face = cv.resize(face, (160, 160))\n",
    "        emb = get_embeddings(face, facenet_model)\n",
    "        \n",
    "        # Classificando a face em label 0 (maskon) ou label 1 (maskoff)\n",
    "        label = get_label(emb, mlp_model)\n",
    "\n",
    "        # Caso uma pessoa sem máscara seja identificada, o contador é incrementado\n",
    "        if not label: no_mask_count += 1\n",
    "\n",
    "        # Configurando o tipo e o tamanho da fonte para iniciar as marcações na imagem\n",
    "        font = cv.FONT_HERSHEY_TRIPLEX\n",
    "        font_scale = 0.5\n",
    "\n",
    "        # Desenhando um retângulo em torno da face\n",
    "        # img = cv.rectangle(img, (x1, y1), (x2, y2), color[label], 2)\n",
    "        frame = cv.rectangle(frame, (x1, y1), (x2, y2), color[label], 2)\n",
    "        \n",
    "        \n",
    "        # Escrevendo a classificação MASK ou NO MASK em cima do retângulo desenhado\n",
    "        # Essa descrição é baseada no label\n",
    "        cv.putText(frame, label_description[label], (x1, y1-10), font, fontScale=font_scale,\n",
    "                    color=color[label], thickness=1)\n",
    "\n",
    "        # Escrevendo no topo do frame um informativo sobre quantas pessoas estão sem máscara na imagem\n",
    "        cv.putText(frame, f'People without mask: {no_mask_count}', (15, 15), font, fontScale=0.6,\n",
    "                    color=(0, 0, 0), thickness=1)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectando o uso de máscara em um vídeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando captura...\n",
      "\n",
      "Captura iniciada!\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001FDD37260D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001FDD37260D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001FDD3B8A9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001FDD3B8A9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001FDD3B8A828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001FDD3B8A828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FDD3B8A828> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001FDD3A798B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001FDD3A798B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "print('Iniciando captura...\\n')\n",
    "\n",
    "# Instanciando um objeto VideoCapture, para selecionar sua webcam\n",
    "# Também é possível renderizar um vídeo pronto, basta você passar o caminho desse arquivo no lugar do parâmetro 0\n",
    "vid = cv.VideoCapture(0)\n",
    "\n",
    "print('Captura iniciada!')\n",
    "\n",
    "# A captura dos frames através da sua webcam ou vídeo será feita até que você pressione a tecla 'q'\n",
    "while (True):\n",
    "\n",
    "    # Lendo a imagem e extraindo o frame\n",
    "    # O método read() retorna dois resultados. O primeiro diz se a captura do frame foi feita com sucesso ou não, enquanto o segundo entrega o frame capturado.\n",
    "    _, frame = vid.read()\n",
    "\n",
    "    # Fazendo a detecção de máscara e as marcações nas faces identificadas no frame\n",
    "    detec = mark_points_in_frame(frame)\n",
    "\n",
    "    # Exibindo o frame resultante do método anterior\n",
    "    cv.imshow('frame', detec)\n",
    "\n",
    "    # Condição de parada do loop: pressione a tecla 'q'\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Fechando o arquivo de vídeo ou dispositivo de captura\n",
    "vid.release()\n",
    "\n",
    "# Fechando as janelas abertas\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# Apagando o objeto da memória\n",
    "del (vid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mini",
   "language": "python",
   "name": "mini"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
